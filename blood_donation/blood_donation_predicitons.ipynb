{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Transfusion Service Center\n",
    "\n",
    "\n",
    "Data Set Characteristics:   Multivariate\n",
    "\n",
    "Number of Instances: 748\n",
    "\n",
    "Area: Business\n",
    "\n",
    "Attribute Characteristics: Real\n",
    "\n",
    "Number of Attributes: 5\n",
    "\n",
    "Date Donated 2008-10-03\n",
    "\n",
    "Associated Tasks: Classification\n",
    "\n",
    "Number of Web Hits:\n",
    "\n",
    "140894\n",
    "\n",
    "#### Source:\n",
    "\n",
    "Original Owner and Donor \n",
    "Prof. I-Cheng Yeh \n",
    "Department of Information Management \n",
    "Chung-Hua University, \n",
    "Hsin Chu, Taiwan 30067, R.O.C. \n",
    "e-mail:icyeh '@' chu.edu.tw \n",
    "TEL:886-3-5186511 \n",
    "\n",
    "Date Donated: October 3, 2008 \n",
    "\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "To demonstrate the RFMTC marketing model (a modified version of RFM), this study adopted the donor database of Blood Transfusion Service Center in Hsin-Chu City in Taiwan. The center passes their blood transfusion service bus to one university in Hsin-Chu City to gather blood donated about every three months. To build a FRMTC model, we selected 748 donors at random from the donor database. These 748 donor data, each one included R (Recency - months since last donation), F (Frequency - total number of donation), M (Monetary - total blood donated in c.c.), T (Time - months since first donation), and a binary variable representing whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood).\n",
    "\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "Given is the variable name, variable type, the measurement unit and a brief description. The \"Blood Transfusion Service Center\" is a classification problem. The order of this listing corresponds to the order of numerals along the rows of the database. \n",
    "\n",
    "    R (Recency - months since last donation), \n",
    "    F (Frequency - total number of donation), \n",
    "    M (Monetary - total blood donated in c.c.), \n",
    "    T (Time - months since first donation), and \n",
    "    a binary variable representing whether he/she donated blood in March 2007 (1 stand for donating blood; 0 stands for not donating blood). \n",
    "\n",
    "\n",
    "Table 1 shows the descriptive statistics of the data. We selected 500 data at random as the training set, and the rest 248 as the testing set. \n",
    "\n",
    "Table 1. Descriptive statistics of the data \n",
    "\n",
    "    Variable\tData Type\tMeasurement\tDescription\tmin\tmax\tmean\tstd \n",
    "    Recency quantitative\tMonths\tInput\t0.03\t74.4\t9.74\t8.07 \n",
    "    Frequency quantitative\tTimes\tInput\t1\t50\t5.51\t5.84 \n",
    "    Monetary\tquantitative\tc.c. blood\tInput\t250\t12500\t1378.68\t1459.83 \n",
    "    Time quantitative\tMonths\tInput\t2.27\t98.3\t34.42\t24.32 \n",
    "    Whether he/she donated blood in March 2007\tbinary\t1=yes 0=no\tOutput\t0\t1\t1 (24%) 0 (76%)\n",
    "\n",
    "\n",
    "#### Relevant Papers:\n",
    "\n",
    "Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming, \"Knowledge discovery on RFM model using Bernoulli sequence,\" Expert Systems with Applications, 2008, [Web Link]\n",
    "\n",
    "#### Resources:\n",
    "\n",
    "* https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "from functools import lru_cache\n",
    "\n",
    "# standard tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %load_ext autoreload\n",
    "\n",
    "seed = 7 * 9\n",
    "np.random.seed(seed)\n",
    "\n",
    "import xgboost\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale_cols = {}\n",
    "\n",
    "def rename_cols(name):\n",
    "    if '(' in name:\n",
    "        name = name.split('(')[0]\n",
    "    return ''.join(map(lambda x: x[0], name.lower().split()))\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def get_data():\n",
    "    df = pd.read_csv('data/BloodDonation.csv', index_col=0)\n",
    "    test_df = pd.read_csv('data/BloodDonationTest.csv', index_col=0)\n",
    "\n",
    "    df.drop(['Total Volume Donated (c.c.)'], inplace=True, axis=1)\n",
    "    test_df.drop(['Total Volume Donated (c.c.)'], inplace=True, axis=1)\n",
    "    \n",
    "    # rename cols\n",
    "    new_cols_names = df.columns.map(rename_cols)\n",
    "    for old_name, new_name in zip(df.columns, new_cols_names):\n",
    "        print('Rename:', old_name, '\\t\\tNewname:', new_name)\n",
    "    df.columns = new_cols_names\n",
    "    test_df.columns = test_df.columns.map(rename_cols)\n",
    "    \n",
    "    global scale_cols\n",
    "    for col in df.columns[:-1]:\n",
    "        scale_cols[col] = StandardScaler(copy=True, with_mean=True, with_std=True).fit(df[col])\n",
    "        df[col] = scale_cols[col].transform(df[col])\n",
    "        test_df[col] = scale_cols[col].transform(test_df[col])\n",
    "\n",
    "    return (df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Data Modelling\n",
    "def get_test_train(df):\n",
    "    X = df.drop('mdim2', axis=1)\n",
    "    y = df['mdim2']\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=1234)\n",
    "    return (X_train, X_validation, y_train, y_validation)\n",
    "\n",
    "\n",
    "def test_train_validation_splt(X, y):\n",
    "    # https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn\n",
    "    from sklearn.cross_validation import train_test_split as tts\n",
    "    SEED = 2000\n",
    "    x_train, x_validation_and_test, y_train, y_validation_and_test = tts(X, y, test_size=.4, random_state=SEED)\n",
    "    x_validation, x_test, y_validation, y_test = tts(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "    return (x_train, x_test, x_validation,\n",
    "            y_train, y_test, y_validation)\n",
    "\n",
    "\n",
    "## save preds\n",
    "def save_preds(preds, filename='submit.csv'):\n",
    "    pd.DataFrame(preds.astype(np.float64),\n",
    "                 index=test_df.index,\n",
    "                 columns=['Made Donation in March 2007']\n",
    "                ).to_csv(filename)\n",
    "    print('stored file as', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rename: Months since Last Donation \t\tNewname: msld\n",
      "Rename: Number of Donations \t\tNewname: nod\n",
      "Rename: Months since First Donation \t\tNewname: msfd\n",
      "Rename: Made Donation in March 2007 \t\tNewname: mdim2\n"
     ]
    }
   ],
   "source": [
    "df, test_df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msld', 'nod', 'msfd', 'mdim2'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['nod_per_msfd'] = df['nod'] / df['msfd']\n",
    "df['msfd_per_nod'] = 1/df['nod_per_msfd']\n",
    "\n",
    "test_df['nod_per_msfd'] = test_df['nod'] / test_df['msfd']\n",
    "test_df['msfd_per_nod'] = 1/test_df['nod_per_msfd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msld', 'nod', 'msfd', 'mdim2', 'nod_per_msfd', 'msfd_per_nod'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = get_test_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((432, 5), (144, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.7946162014164528, 6.7158953989390273)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(alpha=0.5, binarize=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.5, binarize=0.5, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0787411625971761, 7.9152251013259409)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.ensemble.GradientBoostingClassifier(\n",
    "    warm_start=True, subsample=.8,\n",
    "    n_estimators=500,\n",
    "#     learning_rate=0.0001,\n",
    "    presort=True, verbose=0).fit(X_train, y_train)\n",
    "# log_loss(y, clf.predict(X))\n",
    "\n",
    "# results = cross_val_score(clf, X, y, cv=kfold, scoring='log_loss')\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=500, presort=True, random_state=None,\n",
       "              subsample=0.8, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def special_preprocessing(X):\n",
    "    new_X  = X.copy()\n",
    "    cols = X.columns.tolist()\n",
    "    \n",
    "    # check if its already processed\n",
    "    if len(X.columns) > 5:\n",
    "        return new_X\n",
    "    for col in cols:\n",
    "        new_X['tan_' + col] = np.tan(X[col])\n",
    "    for col1, col2 in itertools.combinations(cols, 2):\n",
    "        new_X['BC_' + col1 + ':' + col2] = 6 * X[col1] - 3 *  X[col2]\n",
    "        new_X['BC1_' + col1 + ':' + col2] =  X[col1] /  X[col2] \n",
    "        new_X['BC2_' + col1 + ':' + col2] = X[col1] * X[col1]\n",
    "    for col1, col2, col3 in itertools.combinations(cols, 3):\n",
    "        new_X['TC_' + col1 + ':' + col2 + ':' + col3] = 5 * X[col1] - 3 *  X[col2] + X[col3] + 12\n",
    "        new_X['TC2_' + col1 + ':' + col2 + ':' + col3] = X[col1] *  X[col2] * X[col3]\n",
    "\n",
    "    return new_X\n",
    "\n",
    "new_X_train = special_preprocessing(X_train).copy()\n",
    "new_X_validation = special_preprocessing(X_validation).copy()\n",
    "# clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=500).fit(new_X, y)\n",
    "# log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf = sklearn.ensemble.GradientBoostingClassifier(\n",
    "    warm_start=True, subsample=.8,\n",
    "    n_estimators=500,\n",
    "#     learning_rate=0.0001,\n",
    "    presort=True, verbose=0).fit(new_X_train, y_train)\n",
    "# log_loss(y, clf.predict(X))\n",
    "\n",
    "# results = cross_val_score(clf, X, y, cv=kfold, scoring='log_loss')\n",
    "log_loss(y_train, clf.predict(new_X_train)), log_loss(y_validation, clf.predict(new_X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.5166304787513178, 5.9963653211780485)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(max_depth=4,\n",
    "                      learning_rate=0.05,\n",
    "                      reg_alpha=0.1,\n",
    "                      reg_lambda=0.5,\n",
    "                      seed=12,\n",
    "#                       eta=0.02,\n",
    "                      colsample_bylevel=0.5,\n",
    "                      objective= 'binary:logistic'\n",
    "#                       n_estimators=800\n",
    "                     )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68503\ttrain-logloss:0.68421\n",
      "Multiple eval metrics have been passed: 'train-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.566094\ttrain-logloss:0.565548\n",
      "[40]\ttrain-logloss:0.498244\ttrain-logloss:0.506693\n",
      "[60]\ttrain-logloss:0.457584\ttrain-logloss:0.480979\n",
      "[80]\ttrain-logloss:0.427996\ttrain-logloss:0.468697\n",
      "[100]\ttrain-logloss:0.407399\ttrain-logloss:0.464057\n",
      "[120]\ttrain-logloss:0.391243\ttrain-logloss:0.462123\n",
      "[140]\ttrain-logloss:0.376362\ttrain-logloss:0.455048\n",
      "[160]\ttrain-logloss:0.365766\ttrain-logloss:0.450661\n",
      "[180]\ttrain-logloss:0.357882\ttrain-logloss:0.449149\n",
      "[200]\ttrain-logloss:0.350592\ttrain-logloss:0.449524\n",
      "[220]\ttrain-logloss:0.343504\ttrain-logloss:0.450387\n",
      "Stopping. Best iteration:\n",
      "[183]\ttrain-logloss:0.356856\ttrain-logloss:0.448933\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.5166304787513178, 5.9963653211780485)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = xgboost\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 5\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_validation, label=y_validation)\n",
    "\n",
    "watchlist = [(d_train, 'train'),\n",
    "            (d_test, 'train')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.1156553081215366, 7.1956394959656444)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(5, 5, 5), max_iter=500)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9581951939669144, 10.793534206207546)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "clf = MLPClassifier(hidden_layer_sizes=(30, 18, 12, 5),\n",
    "                    max_iter=1250,\n",
    "                    solver='lbfgs', # 'lbfgs', 'adam'\n",
    "                    learning_rate_init=0.01,\n",
    "                    learning_rate='adaptive',\n",
    "                    activation='tanh',\n",
    "                    alpha=0.4,\n",
    "                    validation_fraction=0.25,\n",
    "                    early_stopping=True,\n",
    "                    verbose=True,\n",
    "                    random_state=7)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "log_loss(y_train, clf.predict(X_train)), log_loss(y_validation, clf.predict(X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier, cv, CatboostIpythonWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    custom_loss=['Logloss'],\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.4379004216740072, 6.4760816544050046)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features_indices = np.where(X_train.dtypes != np.float)[0]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_validation, y_validation),\n",
    "#     verbose=True,  # you can uncomment this for text output\n",
    "#     plot=True\n",
    ")\n",
    "\n",
    "log_loss(y_train, model.predict(X_train)), log_loss(y_validation, model.predict(X_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.63126560618864591, 0.70500663543336684)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='tanh', input_dim=5))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='tanh'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(X_train.values, y_train.values, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "log_loss(y_train.values, model.predict(X_train.values)), log_loss(y_validation.values, model.predict(X_validation.values))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_preds(model.predict(test_df.values), 'keras_submit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42249499242623439, 0.47635158023331314)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(8, input_dim=(5)),\n",
    "    Dense(6),\n",
    "    Activation('tanh'),\n",
    "#     Dense(6),\n",
    "#     Activation('relu'),\n",
    "    Dense(6),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train.values, y_train.values, epochs=1000, batch_size=32, verbose=0)\n",
    "\n",
    "log_loss(y_train.values, model.predict(X_train.values)), log_loss(y_validation.values, model.predict(X_validation.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(9, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "# \tmodel.add(Dense(5))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.73735096146938972, 0.63999319029971957)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train.values, y_train.values, cv=kfold)\n",
    "\n",
    "log_loss(y_train.values, model.predict(X_train.values)), log_loss(y_validation.values, model.predict(X_validation.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79545455,  0.79545456,  0.8139535 ,  0.83720931,  0.79069768,\n",
       "        0.79069768,  0.69767443,  0.76744187,  0.67441862,  0.86046512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
