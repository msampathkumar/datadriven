{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "Using the data gathered from Taarifa and the Tanzanian Ministry of Water, can we predict which pumps are functional, which need some repairs, and which don't work at all? Predicting one of these three classes based and a smart understanding of which waterpoints will fail, can improve the maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\n",
    "__Goal: To set a benchmark for improving the data quality and find a best suited algorithm.__\n",
    "\n",
    "For more details please check [Github Repo][2]\n",
    "\n",
    "[1]: https://www.drivendata.org/competitions/7/ \"Link to Competetion Page\"\n",
    "[2]: https://github.com/msampathkumar/datadriven_pumpit \"User Code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.tools import data_transformations, df_check_stats, game, sam_pickle_save, check_metric\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "np.random.seed(69572)\n",
    "plt.style.use('ggplot')\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame Shape: (59400, 39) TotColumns: 39 ObjectCols: 0\n",
      "Data Frame Shape: (59400, 1) TotColumns: 1 ObjectCols: 0\n",
      "Data Frame Shape: (14850, 39) TotColumns: 39 ObjectCols: 0\n"
     ]
    }
   ],
   "source": [
    "# data collection\n",
    "RAW_X = pd.read_csv('data/traning_set_values.csv', index_col='id')\n",
    "RAW_y = pd.read_csv('data/training_set_labels.csv', index_col='id')\n",
    "RAW_TEST_X = pd.read_csv('data/test_set_values.csv', index_col='id')\n",
    "\n",
    "df_check_stats(RAW_X, RAW_y, RAW_TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE PREFIX USED:  tmp/Iteration0_\n"
     ]
    }
   ],
   "source": [
    "# bool columns\n",
    "tmp = ['public_meeting', 'permit']\n",
    "RAW_X[tmp] = RAW_X[tmp].fillna(True)\n",
    "RAW_TEST_X[tmp] = RAW_TEST_X[tmp].fillna(True)\n",
    "\n",
    "# object columns list\n",
    "obj_cols = RAW_X.dtypes[RAW_X.dtypes == 'O'].index.tolist()\n",
    "\n",
    "# object columns\n",
    "RAW_X[obj_cols] = RAW_X[obj_cols].fillna('Other')\n",
    "RAW_TEST_X[obj_cols] = RAW_TEST_X[obj_cols].fillna('Other')\n",
    "\n",
    "# Just assining new names to transformed dataframe pointers\n",
    "X, y, TEST_X = data_transformations(RAW_X, RAW_y, RAW_TEST_X)\n",
    "\n",
    "sam_pickle_save(X, y, TEST_X, prefix=\"tmp/Iteration0_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Test Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.543075196409 F1 Score: 0.543075196409\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sampathm/miniconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>14850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class 0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class 1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1079.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class 2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5706.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  precision  recall  support\n",
       "avg / total      0.38       0.29    0.54  14850.0\n",
       "class 0          0.70       0.54    1.00   8065.0\n",
       "class 1          0.00       0.00    0.00   1079.0\n",
       "class 2          0.00       0.00    0.00   5706.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "AC Score: 0.543097643098 F1 Score: 0.543097643098\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining Scores')\n",
    "_ = check_metric(clf.predict(X_train), y_train)\n",
    "print('\\nTesting Scores')\n",
    "_ = check_metric(clf.predict(X_test), y_test, show_cm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.984848484848 F1 Score: 0.984848484848\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.799865319865 F1 Score: 0.799865319865\n"
     ]
    }
   ],
   "source": [
    "# benchmark - rf\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='rf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
