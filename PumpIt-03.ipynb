{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "Using the data gathered from Taarifa and the Tanzanian Ministry of Water, can we predict which pumps are functional, which need some repairs, and which don't work at all? Predicting one of these three classes based and a smart understanding of which waterpoints will fail, can improve the maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\n",
    "This is also an intermediate-level competition by [DataDriven][1]! All code & support scripts are in [Github Repo][2]\n",
    "\n",
    "_Goal: To do all basic transformation required for further processing Algorithmn selection, Parameter Tuning and if possible threshold check using ROC curve._\n",
    "\n",
    "[1]: https://www.drivendata.org/competitions/7/ \"Link to Competetion Page\"\n",
    "[2]: https://github.com/msampathkumar/datadriven_pumpit \"User Code\"\n",
    "\n",
    "\n",
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.tools import game\n",
    "from scripts.sam_custom_labeler import CUST_CATEGORY_LABELER\n",
    "from scripts.tools import check_metric, data_transformations, df_check_stats\n",
    "from scripts.tools import sam_pickle_load, sam_pickle_save\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "np.random.seed(69572)\n",
    "plt.style.use('ggplot')\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "crazy_list = dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Data Transformation\n",
    "\n",
    "\n",
    "From Data Analysis, list of columns Selected for removing\n",
    "\n",
    "\n",
    " * `scheme_name` has too many null values to fix.\n",
    " * `longitude` has zeros which is not possible.\n",
    " * `public_meetings, permit, amount_tsh, gps_height, population, constructrion_year` columns required interfilling of data has lots of outliers(as zeros)\n",
    " * `wpt_name, ward, subvillage, schema_name, installer, funder` has lots of categorical values\n",
    "\n",
    "Few columns which seems to hold simmilar kind of information\n",
    "\n",
    " * `extraction_type, extraction_type_group, extraction_type_class`\n",
    " * `management, management_group`\n",
    " * `scheme_management, scheme_name`\n",
    " * `payment, payment_type`\n",
    " * `water_quality, quality_group`\n",
    " * `source, source_type, source_class`\n",
    " * `waterpoint_type, waterpoint_type_group`\n",
    " \n",
    " \n",
    " \n",
    "__ Geo Location information: __ All following parameter are availble for same reason, to find the address.\n",
    "\n",
    " * `longitude, latitude`\n",
    " * `region`(region_code)\n",
    " * `district_code within region`\n",
    " * `ward`\n",
    " * `subvillage`\n",
    "\n",
    "Compared to all other columns `regions` columns has complete data(no nulls)\n",
    "\n",
    "\n",
    "## Load Data required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV Files.\n",
      "Dropped following cols ['scheme_name', 'recorded_by', 'amount_tsh', 'num_private', 'region_code', 'district_code', 'wpt_name', 'subvillage', 'ward', 'lga', 'extraction_type_class', 'extraction_type_group', 'management_group', 'payment', 'water_quality', 'source_type', 'source_class', 'waterpoint_type_group']\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# clean up\n",
    "for each in dir():\n",
    "    if each not in crazy_list:\n",
    "        del each\n",
    "\n",
    "########################################################################\n",
    "# data collection\n",
    "RAW_X = pd.read_csv('data/traning_set_values.csv', index_col='id')\n",
    "RAW_y = pd.read_csv('data/training_set_labels.csv', index_col='id')\n",
    "RAW_TEST_X = pd.read_csv('data/test_set_values.csv', index_col='id')\n",
    "print('Loaded CSV Files.')\n",
    "\n",
    "########################################################################\n",
    "# ward, wpt_name, subvillage, wpt_name too many values counts\n",
    "# amount_tsh: too many nulls\n",
    "# num_private: dont know about private pumps\n",
    "\n",
    "drop_columns = '''\n",
    "scheme_name\n",
    "recorded_by\n",
    "amount_tsh\n",
    "num_private\n",
    "region_code\n",
    "district_code\n",
    "wpt_name\n",
    "subvillage\n",
    "ward\n",
    "lga\n",
    "extraction_type_class\n",
    "extraction_type_group\n",
    "management_group\n",
    "payment\n",
    "water_quality\n",
    "source_type\n",
    "source_class\n",
    "waterpoint_type_group\n",
    "'''.strip().splitlines()\n",
    "\n",
    "RAW_X.drop(drop_columns, inplace=True, axis=1)\n",
    "RAW_TEST_X.drop(drop_columns, inplace=True, axis=1)\n",
    "print('Dropped following cols', drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime\n",
    "\n",
    "Generally date is a composition of 3 details, Year + Month + Day. In generally, overtime logically these possible with either increase or decrease based on the condition governing bodies and their state of growth. So, to make it easy for classifier to identify them, we are seperating them into following columns\n",
    "\n",
    "* column for storing year\n",
    "* column for storing month\n",
    "* columns for number of days, since a specific point of date.(similar to Epoch date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# date_recorded -- convert it to datetime format\n",
    "strptime = datetime.datetime.strptime\n",
    "\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "REFERENCE_DATE_POINT = strptime('2014-01-01', DATE_FORMAT)\n",
    "\n",
    "f = lambda x: strptime(str(x), DATE_FORMAT)\n",
    "RAW_X.date_recorded = RAW_X.date_recorded.apply(f)\n",
    "RAW_TEST_X.date_recorded = RAW_TEST_X.date_recorded.apply(f)\n",
    "\n",
    "f = lambda x: x.month\n",
    "RAW_X['date_recorded_month'] = RAW_X.date_recorded.apply(f)\n",
    "RAW_TEST_X['date_recorded_month'] = RAW_TEST_X.date_recorded.apply(f)\n",
    "\n",
    "f = lambda x: (x - REFERENCE_DATE_POINT).days\n",
    "RAW_X.date_recorded = RAW_X.date_recorded.apply(f)\n",
    "RAW_TEST_X.date_recorded = RAW_TEST_X.date_recorded.apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Int Cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude and Latitude\n",
    "\n",
    "During the Analysis stage, we have seen that `logitude` is having zeros and as per Google's map, null coordinates(0, 0) represet a place outside Africa, in sea which is not possible.\n",
    "\n",
    "As we have a region parameter, to identify `logitude` suitable and use it. Please use plot comment below for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Longitude & Latitude -- zero values fix\n",
    "\n",
    "aa = RAW_X['longitude latitude region'.split()].copy()\n",
    "aa = aa[aa.longitude > 5]\n",
    "\n",
    "bb = aa.groupby(by=['region']).mean()\n",
    "bb.columns = ['longitude_mean', 'latitude_mean']\n",
    "cc = aa.groupby(by=['region']).min()\n",
    "cc.columns = ['longitude_min', 'latitude_min']\n",
    "dd = aa.groupby(by=['region']).max()\n",
    "dd.columns = ['longitude_max', 'latitude_max']\n",
    "\n",
    "abcd = bb.join(cc).join(dd)[['latitude_max',\n",
    " 'latitude_mean',\n",
    " 'latitude_min',\n",
    " 'longitude_max',\n",
    " 'longitude_mean',\n",
    " 'longitude_min']].copy()\n",
    "\n",
    "# abcd.plot()\n",
    "\n",
    "abcde = dict(abcd[['latitude_mean', 'longitude_mean']].T).items()\n",
    "\n",
    "long_lat_reg_dict = {each[0]: {'longitude': each[1][0], 'latitude': each[1][0]} for each in abcde}\n",
    "\n",
    "def f(row):\n",
    "#     print (row)\n",
    "    try:\n",
    "        if (row['longitude'] < 1):\n",
    "            reg = row['region']\n",
    "            row['longitude'] = long_lat_reg_dict[reg]['longitude']\n",
    "#             row['latitude'] = long_lat_reg_dict[reg]['latitude']\n",
    "    except KeyError:\n",
    "        print('KeyError: An expected key(region/longitude/latitude) is missing!')\n",
    "    return row\n",
    "\n",
    "RAW_X = RAW_X.apply(f, axis=1)\n",
    "RAW_TEST_X = RAW_TEST_X.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: only `logitude` is being replaced by function `f(row)`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame Shape: (59400, 22) TotColumns: 22 ObjectCols: 0\n"
     ]
    }
   ],
   "source": [
    "df_check_stats(RAW_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cols\n",
    "\n",
    "We were able to find city poulation details of states of Tanzania and as population needs are in generally has effects of consumption of water usages, we included them to add two new columns.\n",
    "\n",
    "* Population02 - Population in 2002\n",
    "* Population12 - Population in 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## Area & Population\n",
    "# Source: https://www.citypopulation.de/Tanzania-Cities.html\n",
    "\n",
    "header = '''Name\n",
    "Abbr.\n",
    "Status\n",
    "Capital\n",
    "Area\n",
    "Population_1978-08-26\n",
    "Population_1988-08-27\n",
    "Population_2002-08-01\n",
    "Population_2012-08-26\n",
    "'''.lower().strip().splitlines()\n",
    "\n",
    "data = '''Arusha\tARU\tReg\tArusha\t37,576\t...\t744,479\t1,288,088\t1,694,310\n",
    "Dar es Salaam\tDAR\tReg\tDar es Salaam\t1,393\t843,090\t1,360,850\t2,487,288\t4,364,541\n",
    "Dodoma\tDOD\tReg\tDodoma\t41,311\t972,005\t1,235,328\t1,692,025\t2,083,588\n",
    "Geita\tGEI\tReg\tGeita\t20,054\t...\t...\t1,337,718\t1,739,530\n",
    "Iringa\tIRI\tReg\tIringa\t35,503\t...\t...\t840,404\t941,238\n",
    "Kagera\tKAG\tReg\tBukoba\t25,265\t...\t...\t1,791,451\t2,458,023\n",
    "Katavi\tKAT\tReg\tMpanda\t45,843\t...\t...\t408,609\t564,604\n",
    "Kigoma\tKIG\tReg\tKigoma\t37,040\t648,941\t856,770\t1,674,047\t2,127,930\n",
    "Kilimanjaro\tKIL\tReg\tMoshi\t13,250\t902,437\t1,104,673\t1,376,702\t1,640,087\n",
    "Lindi\tLIN\tReg\tLindi\t66,040\t527,624\t646,494\t787,624\t864,652\n",
    "Manyara\tMAY\tReg\tBabati\t44,522\t...\t603,691\t1,037,605\t1,425,131\n",
    "Mara\tMAR\tReg\tMusoma\t21,760\t723,827\t946,418\t1,363,397\t1,743,830\n",
    "Mbeya\tMBE\tReg\tMbeya\t60,350\t1,079,864\t1,476,278\t2,063,328\t2,707,410\n",
    "Morogoro\tMOR\tReg\tMorogoro\t70,624\t939,264\t1,220,564\t1,753,362\t2,218,492\n",
    "Mtwara\tMTW\tReg\tMtwara\t16,710\t771,818\t889,100\t1,124,481\t1,270,854\n",
    "Mwanza\tMWA\tReg\tMwanza\t9,467\t...\t...\t2,058,866\t2,772,509\n",
    "Njombe\tNJO\tReg\tNjombe\t21,347\t...\t...\t648,464\t702,097\n",
    "Pwani\tPWA\tReg\tDar es Salaam\t32,547\t516,586\t636,103\t885,017\t1,098,668\n",
    "Rukwa\tRUK\tReg\tSumbawanga\t22,792\t...\t...\t729,060\t1,004,539\n",
    "Ruvuma\tRUV\tReg\tSongea\t63,669\t561,575\t779,875\t1,113,715\t1,376,891\n",
    "Shinyanga\tSHI\tReg\tShinyanga\t18,901\t...\t...\t1,249,226\t1,534,808\n",
    "Simiyu\tSIM\tReg\tBariadi\t25,212\t...\t...\t1,317,879\t1,584,157\n",
    "Singida\tSIN\tReg\tSingida\t49,340\t613,949\t792,387\t1,086,748\t1,370,637\n",
    "Tabora\tTAB\tReg\tTabora\t76,150\t817,907\t1,036,150\t1,710,465\t2,291,623\n",
    "Tanga\tTAN\tReg\tTanga\t26,677\t1,037,767\t1,280,212\t1,636,280\t2,045,205\n",
    "Zanzibar\tZAN\tSt\tZanzibar\t2,460\t476,111\t640,685\t981,754\t1,303,569\n",
    "'''.strip().splitlines()\n",
    "\n",
    "# data to dataframe\n",
    "df = pd.DataFrame([each.split('\\t') for each in data])\n",
    "df.columns = header\n",
    "\n",
    "# columns\n",
    "df.area = df.area.apply(lambda x: int(x.replace(',', '')))\n",
    "df['population_2002-08-01'] = df['population_2002-08-01'].apply(lambda x: int(x.replace(',', '')))\n",
    "df['population_2012-08-26'] = df['population_2012-08-26'].apply(lambda x: int(x.replace(',', '')))\n",
    "\n",
    "# drop columns\n",
    "df.drop(['abbr.', 'capital', 'status', 'population_1978-08-26', 'population_1988-08-27'], axis=1, inplace=True)\n",
    "\n",
    "# df\n",
    "\n",
    "# checking if we have all regions info\n",
    "mm = RAW_X.region.unique()\n",
    "nn = df.name.tolist()\n",
    "\n",
    "if [ _ for _ in mm if _ not in nn]:\n",
    "    print('found some missing values');\n",
    "    assert False\n",
    "        \n",
    "area_pop = dict()\n",
    "\n",
    "for i, each in df.T.iteritems():\n",
    "    area_pop[each['name']] = { 'area': each['area'],\n",
    "                               'population_2002': each['population_2002-08-01'],\n",
    "                               'population_2012': each['population_2012-08-26']}\n",
    "\n",
    "def fill_area(region):\n",
    "    if region in area_pop:\n",
    "        return area_pop[region]['area']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fill_pop02(region):\n",
    "    if region in area_pop:\n",
    "        return area_pop[region]['population_2002']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fill_pop12(region):\n",
    "    if region in area_pop:\n",
    "        return area_pop[region]['population_2012']\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "########################################################################\n",
    "# Area & Population 2002 & Population 2012\n",
    "RAW_X['Area'] = RAW_X.region.apply(fill_area)\n",
    "RAW_X['Population02'] = RAW_X.region.apply(fill_pop02)\n",
    "RAW_X['Population12'] = RAW_X.region.apply(fill_pop12)\n",
    "\n",
    "RAW_TEST_X['Area'] = RAW_TEST_X.region.apply(fill_area)\n",
    "RAW_TEST_X['Population02'] = RAW_TEST_X.region.apply(fill_pop02)\n",
    "RAW_TEST_X['Population12'] = RAW_TEST_X.region.apply(fill_pop12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061.304017952554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_X.gps_height[RAW_X.gps_height > 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sampathm/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/sampathm/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# GPS Height\n",
    "RAW_TEST_X.gps_height[RAW_TEST_X.gps_height < 1] = RAW_X.gps_height[RAW_X.gps_height > 1].mean()\n",
    "RAW_X.gps_height[RAW_X.gps_height < 1] = RAW_X.gps_height[RAW_X.gps_height > 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Reducing geo location precision to 11 meters\n",
    "LONG_LAT_PRECISION = 0.00001\n",
    "fns_lola =lambda x: (x // LONG_LAT_PRECISION) * LONG_LAT_PRECISION\n",
    "# Reducing Precision of Lat.\n",
    "RAW_X.longitude = RAW_X.longitude.apply(fns_lola)\n",
    "RAW_X.latitude = RAW_X.latitude.apply(fns_lola)\n",
    "RAW_TEST_X.longitude = RAW_TEST_X.longitude.apply(fns_lola)\n",
    "RAW_TEST_X.latitude = RAW_TEST_X.latitude.apply(fns_lola)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# reducing the unique count\n",
    "RAW_X.construction_year = RAW_X.construction_year // 4\n",
    "RAW_TEST_X.construction_year = RAW_TEST_X.construction_year // 4\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# bool columns\n",
    "tmp = ['public_meeting', 'permit']\n",
    "for col in tmp:\n",
    "    RAW_X[col] = RAW_X[col].fillna(False)\n",
    "    RAW_TEST_X[col] = RAW_TEST_X[col].fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obj Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Trans funder\n",
      "Text Trans installer\n",
      "Text Trans basin\n",
      "Text Trans region\n",
      "Text Trans scheme_management\n",
      "Text Trans extraction_type\n",
      "Text Trans management\n",
      "Text Trans payment_type\n",
      "Text Trans quality_group\n",
      "Text Trans quantity\n",
      "Text Trans quantity_group\n",
      "Text Trans source\n",
      "Text Trans waterpoint_type\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# object columns list\n",
    "obj_cols = RAW_X.dtypes[RAW_X.dtypes == 'O'].index.tolist()\n",
    "\n",
    "def text_normalisation(text):\n",
    "    \"\"\"Simplify the text formats.\n",
    "    \n",
    "    * strip trailing leading space\n",
    "    * conver all text to lower cases\n",
    "    * convert nan or None to 'other'\n",
    "    \"\"\"\n",
    "    if text:\n",
    "        text = str(text).strip().lower()\n",
    "        return text\n",
    "    return 'other'\n",
    "\n",
    "for col in obj_cols:\n",
    "    RAW_X[col] = RAW_X[col].apply(text_normalisation)\n",
    "    RAW_TEST_X[col] = RAW_TEST_X[col].apply(text_normalisation)\n",
    "    print ('Text Trans', col)\n",
    "\n",
    "# object columns\n",
    "RAW_X[obj_cols] = RAW_X[obj_cols].fillna('other')\n",
    "RAW_TEST_X[obj_cols] = RAW_TEST_X[obj_cols].fillna('other')\n",
    "\n",
    "RAW_X.scheme_management.replace(\"none\", \"\", inplace=True)\n",
    "RAW_TEST_X.scheme_management.replace(\"none\", \"\", inplace=True)\n",
    "\n",
    "# \"other - mkulima/shinyanga\"(Not present in test) to \"Other\"\n",
    "RAW_X.extraction_type  = RAW_X.extraction_type.replace(\"other - mkulima/shinyanga\", \"other\")\n",
    "RAW_TEST_X.extraction_type  = RAW_TEST_X.extraction_type.replace(\"other - mkulima/shinyanga\", \"other\")\n",
    "\n",
    "# district_code, lga, region_code, may also be a proxy for region.\n",
    "# num_private, no idea of what it is for\n",
    "# recorded_by, has no entropy\n",
    "# wpt_name subvillage ward scheme_name -- too many to handle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obj Labeler\n",
    "\n",
    "`CUST_CATEGORY_LABELER` is a custom labler desined to minimise the number of groups(entrpy) in the data with out much loss of information.\n",
    "\n",
    "For example, for a columns like `funder` has around 1500+ groups. With custom labler, we can check and found that 95% of the data is covered by less than 600 groups while remaining ~1000 groups cover only 5% of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------- installer\n",
      "75 percentage of DATA coverage mean, 59 (in number) groups\n",
      "1936 60\n",
      "75 percentage of DATA coverage mean, 59 (in number) groups\n",
      "980 60\n",
      "-------------------------------------------------------- funder\n",
      "75 percentage of DATA coverage mean, 72 (in number) groups\n",
      "1898 73\n",
      "75 percentage of DATA coverage mean, 72 (in number) groups\n",
      "981 73\n",
      "(59400, 25) (14850, 25) True\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# custom labler\n",
    "# - for reducing entropy with less loss of data.\n",
    "\n",
    "custom_labler = defaultdict(CUST_CATEGORY_LABELER)\n",
    "\n",
    "# these are the data coverages we are expecting\n",
    "#  '<column name>' : <% of data is to be covered>\n",
    "\n",
    "tmp = { 'funder': 75,\n",
    "  'installer': 75,\n",
    "  'wpt_name': 75,\n",
    "#   'subvillage': 75,\n",
    "#   'ward': 75,\n",
    "#   'scheme_name': 75,\n",
    "  }\n",
    "\n",
    "for col, limit  in tmp.items():\n",
    "    if col not in obj_cols:\n",
    "        continue\n",
    "    print('--------------------------------------------------------', col)\n",
    "    labler = custom_labler[col]\n",
    "    labler.DATA_COVERAGE_LIMIT = limit\n",
    "    labler.fit(RAW_X[col])\n",
    "    tmp = labler.fit_transform(RAW_X[col])\n",
    "    print(len(RAW_X[col].value_counts()), len(tmp.value_counts()))\n",
    "    RAW_X[col] = tmp\n",
    "    tmp = labler.etransform(RAW_TEST_X[col])\n",
    "    print(len(RAW_TEST_X[col].value_counts()), len(tmp.value_counts()))\n",
    "\n",
    "print(RAW_X.shape, RAW_TEST_X.shape, all(RAW_X.columns == RAW_TEST_X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, TEST_X = data_transformations(RAW_X, RAW_y, RAW_TEST_X, pickle_path='tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame Shape: (59400, 25) TotColumns: 25 ObjectCols: 0\n",
      "Numpy Array Size: 59400\n",
      "Data Frame Shape: (14850, 25) TotColumns: 25 ObjectCols: 0\n",
      "SAVE PREFIX USED:  tmp/Iteration2_final_\n"
     ]
    }
   ],
   "source": [
    "# Saving the transformed data sources\n",
    "df_check_stats(X, y, TEST_X)\n",
    "sam_pickle_save(X, y, TEST_X, prefix=\"tmp/Iteration2_final_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: End of Data Transformations\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EODT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alogrithm Selection\n",
    "\n",
    "\n",
    "__Conventions for Alogrithmns__\n",
    "\n",
    "* dc: dummy classifier - with strategy as most_frequent(benchmark)\n",
    "* rf: random forest trees\n",
    "* gb: gradient boosting trees\n",
    "* knn: k nearest neightbour(n=3)\n",
    "* mc_ovo_rf: multi class, one vs one wrapper over random forest\n",
    "* mc_ova_rf: multi class, one vs all wrapper over random forest\n",
    "\n",
    "__Score:__\n",
    "\n",
    "* AC Score - imples Accuracy Score.\n",
    "* F1 Score - F1 Score(micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# clean up\n",
    "for each in dir():\n",
    "    if each not in crazy_list:\n",
    "        del each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD PREFIX USED:  tmp/Iteration2_final_\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Just assining new names to transformed dataframe pointers\n",
    "X, y, TEST_X = sam_pickle_load(prefix='tmp/Iteration2_final_')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.543075196409 F1 Score: 0.543075196409\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.543097643098 F1 Score: 0.543097643098\n"
     ]
    }
   ],
   "source": [
    "# Benchmark\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='dc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.982356902357 F1 Score: 0.982356902357\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.798720538721 F1 Score: 0.798720538721\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.755824915825 F1 Score: 0.755824915825\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.751043771044 F1 Score: 0.751043771044\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Trees\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.788507295174 F1 Score: 0.788507295174\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.705521885522 F1 Score: 0.705521885522\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbour\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.978900112233 F1 Score: 0.978900112233\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.799932659933 F1 Score: 0.799932659933\n"
     ]
    }
   ],
   "source": [
    "# Multiclass One vs One - Random Forest\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='mc_ovo_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.987138047138 F1 Score: 0.987138047138\n",
      "\n",
      "Testing Scores\n",
      "------------------------------------------------\n",
      "AC Score: 0.79898989899 F1 Score: 0.79898989899\n"
     ]
    }
   ],
   "source": [
    "# Multiclass One vs All - Random Forest\n",
    "clf = game(X_train, X_test, y_train, y_test, algo='mc_ova_rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each in dir():\n",
    "    if each not in crazy_list:\n",
    "        del each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
