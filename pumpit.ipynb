{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    PUMP IT UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\n",
    "An interactive course exploring this dataset is currently offered by DataCamp.com!\n",
    "\n",
    "Competition End Date: Jan. 28, 2017, 11:59 p.m.\n",
    "\n",
    "This competition is for learning and exploring, so the deadline may be extended in the future.\n",
    "\n",
    "\n",
    "* [Git Hub Repo](https://github.com/msampathkumar/datadriven_pumpit)\n",
    "* [Git Hub Report](https://github.com/msampathkumar/datadriven_pumpit/blob/master/capstone_proposal.mdown)\n",
    "* [Features Details](https://www.drivendata.org/competitions/7/page/25/)\n",
    "\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. Variance Threshold Dict\n",
    "2. know variance threshold for removed columns in \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "np.random.seed(69572)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext writeandexecute\n",
    "\n",
    "# plt.figure(figsize=(120,10))\n",
    "\n",
    "small = (4,3)\n",
    "mid = (10, 8)\n",
    "large = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Hacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkUP Fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from IPython.core.getipython import get_ipython\n",
    "from IPython.core.magic import (Magics, magics_class,  cell_magic)\n",
    "import sys\n",
    "from StringIO import StringIO\n",
    "from markdown import markdown\n",
    "from IPython.core.display import HTML\n",
    " \n",
    "@magics_class\n",
    "class MarkdownMagics(Magics):\n",
    " \n",
    "    @cell_magic\n",
    "    def asmarkdown(self, line, cell):\n",
    "        buffer = StringIO()\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        try:\n",
    "            exec(cell, locals(), self.shell.user_ns)\n",
    "        except:\n",
    "            sys.stdout = stdout\n",
    "            raise\n",
    "        sys.stdout = stdout\n",
    "        return HTML(\"<p>{}</p>\".format(markdown(buffer.getvalue(), extensions=['markdown.extensions.extra'])))\n",
    "        return buffer.getvalue() + 'test'\n",
    " \n",
    "get_ipython().register_magics(MarkdownMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_markup_value_counts(dataframe, max_print_value_counts=30, show_plots=False, figsize=(9, 3)):\n",
    "    '''\n",
    "    prints value counts of each feature in data frame\n",
    "    '''\n",
    "    if not figsize:\n",
    "        figsize=(9, 3)\n",
    "    mydf = pd.DataFrame.copy(dataframe)\n",
    "    i = 0\n",
    "    raw_markup_data = []\n",
    "    pp = raw_markup_data.append\n",
    "    pp('''|Col ID|Col Name|UniqCount|Col Values|UniqValCount|''')\n",
    "    pp('''|------|--------|---------|----------|------------|''')\n",
    "    for col in mydf.dtypes.index:\n",
    "        i += 1\n",
    "        sam = mydf[col]\n",
    "        sam_value_counts = sam.value_counts()\n",
    "        tmp = len(sam_value_counts)\n",
    "        sam_value_counts_len = len(sam_value_counts)\n",
    "        if 1 < sam_value_counts_len < max_print_value_counts:\n",
    "            flag = True\n",
    "            for key, val in dict(sam_value_counts).iteritems():\n",
    "                if flag:\n",
    "                    pp('|%i|%s|%i|%s|%s|' % (\n",
    "                            i, col, sam_value_counts_len, key, val))\n",
    "                    flag = False\n",
    "                else:\n",
    "                    pp('||-|-|%s|%s|' % (key, val))\n",
    "            if show_plots:\n",
    "                plt.figure(i)\n",
    "                ax = sam_value_counts.plot(kind='barh', figsize=figsize)\n",
    "                _ = plt.title(col.upper())\n",
    "                _ = plt.xlabel('counts')\n",
    "        else:\n",
    "            pp('|%i|%s|%i|||' % (i, col, sam_value_counts_len))\n",
    "    return raw_markup_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')    \n",
    "\n",
    "    \n",
    "def confusion_maxtrix_stuff(y_test, y_pred, class_names):\n",
    "    '''\n",
    "    Example\n",
    "    >>> confusion_maxtrix_stuff(y_test,\n",
    "                                y_pred,\n",
    "                                class_names=RAW_y.status_group.value_counts().keys()\n",
    "                                ):\n",
    "    '''\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RAW_X = pd.read_csv('traning_set_values.csv', index_col='id')\n",
    "RAW_y = pd.read_csv('training_set_labels.csv', index_col='id')\n",
    "RAW_TEST_X = pd.read_csv('test_set_values.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RAW_X.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "* Most of the data seems categorical\n",
    "* Following pairs looks closesly related\n",
    "    * quantity & quantity_group\n",
    "    * quality_group & water_quality\n",
    "    * extraction_type, extraction_type_class & extraction_type_group\n",
    "* Other\n",
    "    * recorded_by, seems to hold only a single value\n",
    "    * population & amount_tsh, values are for some given as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check those group with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%asmarkdown\n",
    "# show me the graphs\n",
    "tmp = raw_markup_value_counts(dataframe=RAW_X, max_print_value_counts=10, show_plots=True, figsize=(9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Num/Bool Tranformations **\n",
    "\n",
    "* date_recorded --> Int\n",
    "* longitude --> Float(less precision)\n",
    "* latitude --> Float(less precision)\n",
    "* public_meeting --> Bool\n",
    "* permit --> Bool\n",
    "\n",
    "\n",
    "Precision Description of Longititude and Latitude is available here at below link.\n",
    "* http://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "strptime = datetime.strptime\n",
    "\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "REFERENCE_DATE_POINT = strptime('2000-01-01', DATE_FORMAT)\n",
    "\n",
    "# Reducing geo location precision to 11 meters\n",
    "LONG_LAT_PRECISION = 0.001\n",
    "\n",
    "def sam_datetime_to_number(x):\n",
    "    return (strptime(str(x), DATE_FORMAT) - REFERENCE_DATE_POINT).days\n",
    "\n",
    "\n",
    "# Transforming Date to Int.\n",
    "if RAW_X.date_recorded.dtype == 'O':\n",
    "    RAW_X.date_recorded = RAW_X.date_recorded.map(sam_datetime_to_number)\n",
    "    RAW_TEST_X.date_recorded = RAW_TEST_X.date_recorded.map(sam_datetime_to_number)\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "_ = np.mean(RAW_X[u'latitude'][RAW_X.latitude < -1.0].values)\n",
    "if not RAW_X.loc[RAW_X.latitude >= -1.0, u'latitude'].empty:\n",
    "    RAW_X.loc[RAW_X.latitude >= -1.0, u'latitude'] = _\n",
    "    RAW_TEST_X.loc[RAW_TEST_X.latitude >= -1.0, u'latitude'] = _\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "_ = np.mean(RAW_X[u'longitude'][RAW_X[u'longitude'] > 1.0].values)\n",
    "if not RAW_X.loc[RAW_X[u'longitude'] <= 1.0, u'longitude'].empty:\n",
    "    RAW_X.loc[RAW_X[u'longitude'] <= 1.0, u'longitude'] = _\n",
    "    RAW_TEST_X.loc[RAW_TEST_X[u'longitude'] <= 1.0, u'longitude'] = _\n",
    "\n",
    "\n",
    "# Reducing Precision of Lat.\n",
    "if RAW_X.longitude.mean() < 50:\n",
    "    RAW_X.longitude = RAW_X.longitude // LONG_LAT_PRECISION\n",
    "    RAW_X.latitude = RAW_X.latitude // LONG_LAT_PRECISION\n",
    "    RAW_TEST_X.longitude = RAW_TEST_X.longitude // LONG_LAT_PRECISION\n",
    "    RAW_TEST_X.latitude = RAW_TEST_X.latitude // LONG_LAT_PRECISION\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "if RAW_X.public_meeting.dtype != 'bool':\n",
    "    RAW_X.public_meeting = RAW_X.public_meeting == True\n",
    "    RAW_TEST_X.public_meeting = RAW_TEST_X.public_meeting == True\n",
    "\n",
    "if RAW_X.permit.dtype != 'bool':\n",
    "    RAW_X.permit = RAW_X.permit == True\n",
    "    RAW_TEST_X.permit = RAW_TEST_X.permit == True\n",
    "\n",
    "\n",
    "# checking\n",
    "if list(RAW_TEST_X.dtypes[RAW_TEST_X.dtypes != RAW_X.dtypes]):\n",
    "    raise Exception('RAW_X.dtypes and RAW_TEST_X.dtypes are not in Sync')\n",
    "else:\n",
    "    print 'All in Good Shape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Text Data Tranformations **\n",
    "\n",
    "We are going to basic clean action like, lower and upper case issue. Clearning of non ascii values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_transformation(name):\n",
    "    if name:\n",
    "        name = name.lower().strip()\n",
    "        name = ''.join([i if 96 < ord(i) < 128 else ' ' for i in name])\n",
    "        if 'and' in name:\n",
    "            name = name.replace('and', ' ')\n",
    "        if '/' in name:\n",
    "            name = name.replace('/', ' ')\n",
    "        while '  ' in name:\n",
    "            name = name.replace('  ', ' ')\n",
    "        return name.strip()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%asmarkdown\n",
    "\n",
    "print '''\n",
    "|Column|Prev.|Current|\n",
    "|------|-----|-------|'''\n",
    "for col in RAW_X.dtypes[RAW_X.dtypes == object].index:\n",
    "    aa = len(RAW_X[col].unique())\n",
    "    RAW_X[col] = RAW_X[col].fillna('').apply(text_transformation)\n",
    "    RAW_TEST_X[col] = RAW_TEST_X[col].fillna('').apply(text_transformation)\n",
    "    bb = len(RAW_X[col].unique())\n",
    "    if aa != bb:\n",
    "        print '|%s|%i|%i|' % (col, aa, bb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# For seperate text data experimentation\n",
    "#  taking our funder information\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "funder_dict =  Counter(RAW_X.funder)\n",
    "tmp = funder_dict.keys()\n",
    "tmp.sort()\n",
    "pickle.dump(funder_dict, open('funder.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder\n",
    "\n",
    "Label Encoder with DefaultDict for quick data transformation\n",
    "http://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "\n",
    "# Labels Fit\n",
    "sam = pd.concat([RAW_X, RAW_TEST_X]).apply(lambda x: d[x.name].fit(x))\n",
    "\n",
    "# Labels Transform - Training Data\n",
    "X = RAW_X.apply(lambda x: d[x.name].transform(x))\n",
    "TEST_X = RAW_TEST_X.apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "le = preprocessing.LabelEncoder().fit(RAW_y[u'status_group'])\n",
    "y = le.transform(RAW_y[u'status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('X.pkl', 'w'))\n",
    "pickle.dump(TEST_X, open('TEST_X.pkl', 'w'))\n",
    "pickle.dump(y, open('y.pkl', 'w'))\n",
    "pickle.dump(d, open('d.pkl', 'w'))\n",
    "pickle.dump(le, open('le.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('X.pkl'))\n",
    "y = pickle.load(open('y.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load this when you are about to do text transformation and submission\n",
    "\n",
    "TEST_X = pickle.load(open('TEST_X.pkl'))\n",
    "d = pickle.load(open('d.pkl'))\n",
    "le = pickle.load(open('le.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold\n",
    "\n",
    "To remove all features that are either one or zero (on or off) in more than 80% of the samples.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n",
    "\n",
    "http://stackoverflow.com/questions/29298973/removing-features-with-low-variance-scikit-learn/34850639#34850639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def get_low_variance_columns(dframe=None, columns=[],\n",
    "                             skip_columns=[], threshold=0.0,\n",
    "                             autoremove=False):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn VarianceThreshold for use on pandas dataframes.\n",
    "    \"\"\"\n",
    "    print(\"Finding low-variance features.\")\n",
    "    removed_features = []\n",
    "    ranking_variance_thresholds = {}\n",
    "    try:\n",
    "        # get list of all the original df columns\n",
    "        all_columns = dframe.columns\n",
    "\n",
    "        # remove `skip_columns`\n",
    "        remaining_columns = all_columns.drop(skip_columns)\n",
    "\n",
    "        # get length of new index\n",
    "        max_index = len(remaining_columns) - 1\n",
    "\n",
    "        # get indices for `skip_columns`\n",
    "        skipped_idx = [all_columns.get_loc(col)\n",
    "                       for col\n",
    "                       in skip_columns]\n",
    "\n",
    "        # adjust insert location by the number of columns removed\n",
    "        # (for non-zero insertion locations) to keep relative\n",
    "        # locations intact\n",
    "        for idx, item in enumerate(skipped_idx):\n",
    "            if item > max_index:\n",
    "                diff = item - max_index\n",
    "                skipped_idx[idx] -= diff\n",
    "            if item == max_index:\n",
    "                diff = item - len(skip_columns)\n",
    "                skipped_idx[idx] -= diff\n",
    "            if idx == 0:\n",
    "                skipped_idx[idx] = item\n",
    "\n",
    "        # get values of `skip_columns`\n",
    "        skipped_values = dframe.iloc[:, skipped_idx].values\n",
    "\n",
    "        # get dataframe values\n",
    "        X = dframe.loc[:, remaining_columns].values\n",
    "\n",
    "        # instantiate VarianceThreshold object\n",
    "        vt = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "        # fit vt to data\n",
    "        vt.fit(X)\n",
    "\n",
    "        # threshold ranking\n",
    "        ranking_variance_thresholds = dict(zip(remaining_columns, vt.variances_))\n",
    "\n",
    "        # get the indices of the features that are being kept\n",
    "        feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "        # remove low-variance columns from index\n",
    "        feature_names = [remaining_columns[idx]\n",
    "                         for idx, _\n",
    "                         in enumerate(remaining_columns)\n",
    "                         if idx\n",
    "                         in feature_indices]\n",
    "\n",
    "        # get the columns to be removed\n",
    "        removed_features = list(np.setdiff1d(remaining_columns,\n",
    "                                             feature_names))\n",
    "        print(\"Found {0} low-variance columns.\"\n",
    "              .format(len(removed_features)))\n",
    "\n",
    "        # remove the columns\n",
    "        if autoremove:\n",
    "            print(\"Removing low-variance features.\")\n",
    "            # remove the low-variance columns\n",
    "            X_removed = vt.transform(X)\n",
    "\n",
    "            print(\"Reassembling the dataframe (with low-variance \"\n",
    "                  \"features removed).\")\n",
    "            # re-assemble the dataframe\n",
    "            dframe = pd.DataFrame(data=X_removed,\n",
    "                                  columns=feature_names)\n",
    "\n",
    "            # add back the `skip_columns`\n",
    "            for idx, index in enumerate(skipped_idx):\n",
    "                dframe.insert(loc=index,\n",
    "                              column=skip_columns[idx],\n",
    "                              value=skipped_values[:, idx])\n",
    "            print(\"Succesfully removed low-variance columns.\")\n",
    "\n",
    "        # do not remove columns\n",
    "        else:\n",
    "            print(\"No changes have been made to the dataframe.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not remove low-variance features. Something \"\n",
    "              \"went wrong.\")\n",
    "        return dframe, [], {}\n",
    "\n",
    "    return dframe, removed_features, ranking_variance_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, removed_features, ranking_variance_thresholds = get_low_variance_columns(dframe=X,\n",
    "                                                                            threshold=(0.8 * (1 - 0.8)),\n",
    "                                                                            autoremove=True)\n",
    "\n",
    "print '\\nLow Variance Columns', removed_features\n",
    "print 'Shape of X is', X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_X.drop(removed_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Shape of TEST_X is', TEST_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best\n",
    "\n",
    "* For regression: f_regression, mutual_info_regression\n",
    "* For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SelectKBest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K_BEST_COLS = 25\n",
    "\n",
    "fit = SelectKBest(score_func=chi2, k=K_BEST_COLS).fit(X, y)\n",
    "cols_names = X.columns\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "ranking_selectkbest = dict(zip(cols_names, fit.scores_))\n",
    "selected_cols =  [_ for _ in cols_names[:K_BEST_COLS]]\n",
    "\n",
    "features = pd.DataFrame(fit.transform(X))\n",
    "features.columns = selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% pprint\n",
    "ranking_selectkbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape, features.shape, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Removed Columns:\\n\\t', '\\n\\t'.join([ _ for _ in X.columns if _ not in features.columns ])\n",
    "\n",
    "print '\\nSelected Columns:\\n\\t', '\\n\\t'.join(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(fit.transform(X))\n",
    "X.columns = selected_cols\n",
    "\n",
    "print\n",
    "\n",
    "# TEST_X = pd.DataFrame(fit.transform(TEST_X))\n",
    "# TEST_X.columns = selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "pca = PCA(n_components=18)\n",
    "fit = pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "_ = plt.scatter (range(len(fit.explained_variance_ratio_)), fit.explained_variance_ratio_.cumsum())\n",
    "\n",
    "_ = plt.xlabel('cumsum of explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pca.transform(X)\n",
    "TEST_X = pca.transform(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Unsupervised Learning Exploration(Gaussian Process, Neural Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixture.GaussianMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=6, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bic = np.array(bic)\n",
    "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                              'darkorange'])\n",
    "clf = best_gmm\n",
    "bars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the BIC scores\n",
    "spl = plt.subplot(2, 1, 1)\n",
    "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                  (i + 1) * len(n_components_range)],\n",
    "                        width=.2, color=color))\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "plt.title('BIC score per model')\n",
    "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "spl.set_xlabel('Number of components')\n",
    "spl.legend([b[0] for b in bars], cv_types)\n",
    "\n",
    "# Plot the winner\n",
    "splot = plt.subplot(2, 1, 2)\n",
    "Y_ = clf.predict(X)\n",
    "\n",
    "for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Selected GMM: full model, 2 components')\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linalg.eigh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Supervised Learning(GBT Trees, Nearest Neighbours, RF, One-vs-One)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print features.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1)\n",
    "# class_weight=\"balanced_subsample\"/\"balanced\"\n",
    "# criterion=\"gini\"/\"entropy\"\n",
    "\n",
    "rf_clf = rf_clf.fit(X_train, y_train)\n",
    "# pred = rf_clf.predict_proba(X_test)\n",
    "rf_clf.score(X_test, y_test)\n",
    "\n",
    "# (n_estimators=100, class_weight=\"balanced_subsample\", n_jobs=-1) 0.80782828282828278\n",
    "# (n_estimators=100, class_weight=\"balanced_subsample\", n_jobs=-1) 0.81186868686868685\n",
    "# (n_estimators=150, class_weight=\"balanced_subsample\", n_jobs=-1) 0.8113636363636364\n",
    "\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81018518518518523\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.80858585858585863\n",
    "# (n_estimators=150, criterion='gini', n_jobs=-1) 0.80942760942760938\n",
    "# (n_estimators=150, criterion='entropy', n_jobs=-1) 0.81060606060606055\n",
    "\n",
    "# (n_estimators=10, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.79781144781144786\n",
    "# (n_estimators=10, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.80185185185185182\n",
    "# (n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.8113636363636364\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.80984848484848482\n",
    "\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.81259259259259264\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81259259259259264\n",
    "# (n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81252525252525254"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    n_estimators: [10, 50, 100],\n",
    "    class_weight: 'balanced_subsample balanced'.split(),\n",
    "    criterion: 'gini entropy'.split(),\n",
    "#     n_jobs: [-1]\n",
    "}\n",
    "GS_CV = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, n_jobs=-1)\n",
    "\n",
    "GS_CV.fit(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "tmp = clf.feature_importances_\n",
    "\n",
    "# making importance relative\n",
    "a, b = min(tmp), max(tmp)\n",
    "cols_imp = (tmp - a) /b\n",
    "_ = plt.scatter(range(30), cols_imp)\n",
    "_ = plt.plot((0, 29), (0.15,0.15), '-r')\n",
    "_ = plt.xlabel('Columns')\n",
    "_ = plt.ylabel('Relative Col Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(lambda x: len(x), [X_test, y_test])\n",
    "\n",
    "clf.score(X_test, y_test) # 0.81097643097643102"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "confusion_maxtrix_stuff(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05).fit(X_train, y_train)\n",
    "\n",
    "gbm_predictions = gbm.predict(X_test)\n",
    "\n",
    "print sum(gbm_predictions == y_test)/ len(y_pred) # 0.7279461279461279\n",
    "\n",
    "\n",
    "confusion_maxtrix_stuff(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids = RAW_TEST_X.index\n",
    "\n",
    "predictions = clf.predict(TEST_X)\n",
    "\n",
    "print (predictions.shape)\n",
    "\n",
    "predictions_labels = le.inverse_transform(predictions)\n",
    "\n",
    "# sub = pd.DataFrame(predictions, columns=list(le.classes_))\n",
    "sub = pd.DataFrame(predictions_labels, columns=['status_group'])\n",
    "sub.head()\n",
    "\n",
    "sub.insert(loc=0, column='id', value=test_ids)\n",
    "sub.reset_index()\n",
    "sub.to_csv('submit.csv', index = False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
