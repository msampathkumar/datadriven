{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PUMP IT UP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Git Hub Repo](https://github.com/msampathkumar/datadriven_pumpit)\n",
    "* [Git Hub Report](https://github.com/msampathkumar/datadriven_pumpit/blob/master/capstone_proposal.mdown)\n",
    "* [Features Details](https://www.drivendata.org/competitions/7/page/25/)\n",
    "\n",
    "\n",
    "TODO:\n",
    "\n",
    "1. Variance Threshold Dict\n",
    "2. know variance threshold for removed columns in \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'writeandexecute' magic loaded.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "np.random.seed(69572)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext writeandexecute\n",
    "\n",
    "# plt.figure(figsize=(120,10))\n",
    "\n",
    "small = (4,3)\n",
    "mid = (10, 8)\n",
    "large = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Hacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MarkUP Fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from IPython.core.getipython import get_ipython\n",
    "from IPython.core.magic import (Magics, magics_class,  cell_magic)\n",
    "import sys\n",
    "from StringIO import StringIO\n",
    "from markdown import markdown\n",
    "from IPython.core.display import HTML\n",
    " \n",
    "@magics_class\n",
    "class MarkdownMagics(Magics):\n",
    " \n",
    "    @cell_magic\n",
    "    def asmarkdown(self, line, cell):\n",
    "        buffer = StringIO()\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        try:\n",
    "            exec(cell, locals(), self.shell.user_ns)\n",
    "        except:\n",
    "            sys.stdout = stdout\n",
    "            raise\n",
    "        sys.stdout = stdout\n",
    "        return HTML(\"<p>{}</p>\".format(markdown(buffer.getvalue(), extensions=['markdown.extensions.extra'])))\n",
    "        return buffer.getvalue() + 'test'\n",
    " \n",
    "get_ipython().register_magics(MarkdownMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_markup_value_counts(dataframe, max_print_value_counts=30, show_plots=False, figsize=(9, 3)):\n",
    "    '''\n",
    "    prints value counts of each feature in data frame\n",
    "    '''\n",
    "    if not figsize:\n",
    "        figsize=(9, 3)\n",
    "    mydf = pd.DataFrame.copy(dataframe)\n",
    "    i = 0\n",
    "    raw_markup_data = []\n",
    "    pp = raw_markup_data.append\n",
    "    pp('''|Col ID|Col Name|UniqCount|Col Values|UniqValCount|''')\n",
    "    pp('''|------|--------|---------|----------|------------|''')\n",
    "    for col in mydf.dtypes.index:\n",
    "        i += 1\n",
    "        sam = mydf[col]\n",
    "        sam_value_counts = sam.value_counts()\n",
    "        tmp = len(sam_value_counts)\n",
    "        sam_value_counts_len = len(sam_value_counts)\n",
    "        if 1 < sam_value_counts_len < max_print_value_counts:\n",
    "            flag = True\n",
    "            for key, val in dict(sam_value_counts).iteritems():\n",
    "                if flag:\n",
    "                    pp('|%i|%s|%i|%s|%s|' % (\n",
    "                            i, col, sam_value_counts_len, key, val))\n",
    "                    flag = False\n",
    "                else:\n",
    "                    pp('||-|-|%s|%s|' % (key, val))\n",
    "            if show_plots:\n",
    "                plt.figure(i)\n",
    "                ax = sam_value_counts.plot(kind='barh', figsize=figsize)\n",
    "                _ = plt.title(col.upper())\n",
    "                _ = plt.xlabel('counts')\n",
    "        else:\n",
    "            pp('|%i|%s|%i|||' % (i, col, sam_value_counts_len))\n",
    "    return raw_markup_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')    \n",
    "\n",
    "    \n",
    "def confusion_maxtrix_stuff(y_test, y_pred, class_names):\n",
    "    '''\n",
    "    Example\n",
    "    >>> confusion_maxtrix_stuff(y_test,\n",
    "                                y_pred,\n",
    "                                class_names=RAW_y.status_group.value_counts().keys()\n",
    "                                ):\n",
    "    '''\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RAW_X = pd.read_csv('traning_set_values.csv', index_col='id')\n",
    "RAW_y = pd.read_csv('training_set_labels.csv', index_col='id')\n",
    "RAW_TEST_X = pd.read_csv('test_set_values.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RAW_X.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Observations: **\n",
    "\n",
    "* Most of the data seems categorical\n",
    "* Following pairs looks closesly related\n",
    "    * quantity & quantity_group\n",
    "    * quality_group & water_quality\n",
    "    * extraction_type, extraction_type_class & extraction_type_group\n",
    "* Other\n",
    "    * recorded_by, seems to hold only a single value\n",
    "    * population & amount_tsh, values are for some given as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check those group with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%asmarkdown\n",
    "# show me the graphs\n",
    "tmp = raw_markup_value_counts(dataframe=RAW_X, max_print_value_counts=10, show_plots=True, figsize=(9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Num/Bool Tranformations **\n",
    "\n",
    "* date_recorded --> Int\n",
    "* longitude --> Float(less precision)\n",
    "* latitude --> Float(less precision)\n",
    "* public_meeting --> Bool\n",
    "* permit --> Bool\n",
    "\n",
    "\n",
    "Precision Description of Longititude and Latitude is available here at below link.\n",
    "* http://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "strptime = datetime.strptime\n",
    "\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "REFERENCE_DATE_POINT = strptime('2000-01-01', DATE_FORMAT)\n",
    "\n",
    "# Reducing geo location precision to 11 meters\n",
    "LONG_LAT_PRECISION = 0.001\n",
    "\n",
    "def sam_datetime_to_number(x):\n",
    "    return (strptime(str(x), DATE_FORMAT) - REFERENCE_DATE_POINT).days\n",
    "\n",
    "\n",
    "# Transforming Date to Int.\n",
    "if RAW_X.date_recorded.dtype == 'O':\n",
    "    RAW_X.date_recorded = RAW_X.date_recorded.map(sam_datetime_to_number)\n",
    "    RAW_TEST_X.date_recorded = RAW_TEST_X.date_recorded.map(sam_datetime_to_number)\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "_ = np.mean(RAW_X[u'latitude'][RAW_X.latitude < -1.0].values)\n",
    "if not RAW_X.loc[RAW_X.latitude >= -1.0, u'latitude'].empty:\n",
    "    RAW_X.loc[RAW_X.latitude >= -1.0, u'latitude'] = _\n",
    "    RAW_TEST_X.loc[RAW_TEST_X.latitude >= -1.0, u'latitude'] = _\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "_ = np.mean(RAW_X[u'longitude'][RAW_X[u'longitude'] > 1.0].values)\n",
    "if not RAW_X.loc[RAW_X[u'longitude'] <= 1.0, u'longitude'].empty:\n",
    "    RAW_X.loc[RAW_X[u'longitude'] <= 1.0, u'longitude'] = _\n",
    "    RAW_TEST_X.loc[RAW_TEST_X[u'longitude'] <= 1.0, u'longitude'] = _\n",
    "\n",
    "\n",
    "# Reducing Precision of Lat.\n",
    "if RAW_X.longitude.mean() < 50:\n",
    "    RAW_X.longitude = RAW_X.longitude // LONG_LAT_PRECISION\n",
    "    RAW_X.latitude = RAW_X.latitude // LONG_LAT_PRECISION\n",
    "    RAW_TEST_X.longitude = RAW_TEST_X.longitude // LONG_LAT_PRECISION\n",
    "    RAW_TEST_X.latitude = RAW_TEST_X.latitude // LONG_LAT_PRECISION\n",
    "\n",
    "\n",
    "# Filling Missing/OUTLIAR Values\n",
    "if RAW_X.public_meeting.dtype != 'bool':\n",
    "    RAW_X.public_meeting = RAW_X.public_meeting == True\n",
    "    RAW_TEST_X.public_meeting = RAW_TEST_X.public_meeting == True\n",
    "\n",
    "if RAW_X.permit.dtype != 'bool':\n",
    "    RAW_X.permit = RAW_X.permit == True\n",
    "    RAW_TEST_X.permit = RAW_TEST_X.permit == True\n",
    "\n",
    "\n",
    "# checking\n",
    "if list(RAW_TEST_X.dtypes[RAW_TEST_X.dtypes != RAW_X.dtypes]):\n",
    "    raise Exception('RAW_X.dtypes and RAW_TEST_X.dtypes are not in Sync')\n",
    "else:\n",
    "    print 'All in Good Shape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Text Data Tranformations **\n",
    "\n",
    "We are going to basic clean action like, lower and upper case issue. Clearning of non ascii values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_transformation(name):\n",
    "    if name:\n",
    "        name = name.lower().strip()\n",
    "        name = ''.join([i if 96 < ord(i) < 128 else ' ' for i in name])\n",
    "        if 'and' in name:\n",
    "            name = name.replace('and', ' ')\n",
    "        if '/' in name:\n",
    "            name = name.replace('/', ' ')\n",
    "        while '  ' in name:\n",
    "            name = name.replace('  ', ' ')\n",
    "        return name.strip()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%asmarkdown\n",
    "\n",
    "print '''\n",
    "|Column|Prev.|Current|\n",
    "|------|-----|-------|'''\n",
    "for col in RAW_X.dtypes[RAW_X.dtypes == object].index:\n",
    "    aa = len(RAW_X[col].unique())\n",
    "    RAW_X[col] = RAW_X[col].fillna('').apply(text_transformation)\n",
    "    RAW_TEST_X[col] = RAW_TEST_X[col].fillna('').apply(text_transformation)\n",
    "    bb = len(RAW_X[col].unique())\n",
    "    if aa != bb:\n",
    "        print '|%s|%i|%i|' % (col, aa, bb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# For seperate text data experimentation\n",
    "#  taking our funder information\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "funder_dict =  Counter(RAW_X.funder)\n",
    "tmp = funder_dict.keys()\n",
    "tmp.sort()\n",
    "pickle.dump(funder_dict, open('funder.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder\n",
    "\n",
    "Label Encoder with DefaultDict for quick data transformation\n",
    "http://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "\n",
    "# Labels Fit\n",
    "sam = pd.concat([RAW_X, RAW_TEST_X]).apply(lambda x: d[x.name].fit(x))\n",
    "\n",
    "# Labels Transform - Training Data\n",
    "X = RAW_X.apply(lambda x: d[x.name].transform(x))\n",
    "TEST_X = RAW_TEST_X.apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "le = preprocessing.LabelEncoder().fit(RAW_y[u'status_group'])\n",
    "y = le.transform(RAW_y[u'status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('X.pkl', 'w'))\n",
    "pickle.dump(TEST_X, open('TEST_X.pkl', 'w'))\n",
    "pickle.dump(y, open('y.pkl', 'w'))\n",
    "pickle.dump(d, open('d.pkl', 'w'))\n",
    "pickle.dump(le, open('le.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('X.pkl'))\n",
    "y = pickle.load(open('y.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load this when you are about to do text transformation and submission\n",
    "\n",
    "TEST_X = pickle.load(open('TEST_X.pkl'))\n",
    "d = pickle.load(open('d.pkl'))\n",
    "le = pickle.load(open('le.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 39), (59400,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold\n",
    "\n",
    "To remove all features that are either one or zero (on or off) in more than 80% of the samples.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance\n",
    "\n",
    "http://stackoverflow.com/questions/29298973/removing-features-with-low-variance-scikit-learn/34850639#34850639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def get_low_variance_columns(dframe=None, columns=[],\n",
    "                             skip_columns=[], threshold=0.0,\n",
    "                             autoremove=False):\n",
    "    \"\"\"\n",
    "    Wrapper for sklearn VarianceThreshold for use on pandas dataframes.\n",
    "    \"\"\"\n",
    "    print(\"Finding low-variance features.\")\n",
    "    removed_features = []\n",
    "    ranking_variance_thresholds = {}\n",
    "    try:\n",
    "        # get list of all the original df columns\n",
    "        all_columns = dframe.columns\n",
    "\n",
    "        # remove `skip_columns`\n",
    "        remaining_columns = all_columns.drop(skip_columns)\n",
    "\n",
    "        # get length of new index\n",
    "        max_index = len(remaining_columns) - 1\n",
    "\n",
    "        # get indices for `skip_columns`\n",
    "        skipped_idx = [all_columns.get_loc(col)\n",
    "                       for col\n",
    "                       in skip_columns]\n",
    "\n",
    "        # adjust insert location by the number of columns removed\n",
    "        # (for non-zero insertion locations) to keep relative\n",
    "        # locations intact\n",
    "        for idx, item in enumerate(skipped_idx):\n",
    "            if item > max_index:\n",
    "                diff = item - max_index\n",
    "                skipped_idx[idx] -= diff\n",
    "            if item == max_index:\n",
    "                diff = item - len(skip_columns)\n",
    "                skipped_idx[idx] -= diff\n",
    "            if idx == 0:\n",
    "                skipped_idx[idx] = item\n",
    "\n",
    "        # get values of `skip_columns`\n",
    "        skipped_values = dframe.iloc[:, skipped_idx].values\n",
    "\n",
    "        # get dataframe values\n",
    "        X = dframe.loc[:, remaining_columns].values\n",
    "\n",
    "        # instantiate VarianceThreshold object\n",
    "        vt = VarianceThreshold(threshold=threshold)\n",
    "\n",
    "        # fit vt to data\n",
    "        vt.fit(X)\n",
    "\n",
    "        # threshold ranking\n",
    "        ranking_variance_thresholds = dict(zip(remaining_columns, vt.variances_))\n",
    "\n",
    "        # get the indices of the features that are being kept\n",
    "        feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "        # remove low-variance columns from index\n",
    "        feature_names = [remaining_columns[idx]\n",
    "                         for idx, _\n",
    "                         in enumerate(remaining_columns)\n",
    "                         if idx\n",
    "                         in feature_indices]\n",
    "\n",
    "        # get the columns to be removed\n",
    "        removed_features = list(np.setdiff1d(remaining_columns,\n",
    "                                             feature_names))\n",
    "        print(\"Found {0} low-variance columns.\"\n",
    "              .format(len(removed_features)))\n",
    "\n",
    "        # remove the columns\n",
    "        if autoremove:\n",
    "            print(\"Removing low-variance features.\")\n",
    "            # remove the low-variance columns\n",
    "            X_removed = vt.transform(X)\n",
    "\n",
    "            print(\"Reassembling the dataframe (with low-variance \"\n",
    "                  \"features removed).\")\n",
    "            # re-assemble the dataframe\n",
    "            dframe = pd.DataFrame(data=X_removed,\n",
    "                                  columns=feature_names)\n",
    "\n",
    "            # add back the `skip_columns`\n",
    "            for idx, index in enumerate(skipped_idx):\n",
    "                dframe.insert(loc=index,\n",
    "                              column=skip_columns[idx],\n",
    "                              value=skipped_values[:, idx])\n",
    "            print(\"Succesfully removed low-variance columns.\")\n",
    "\n",
    "        # do not remove columns\n",
    "        else:\n",
    "            print(\"No changes have been made to the dataframe.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not remove low-variance features. Something \"\n",
    "              \"went wrong.\")\n",
    "        return dframe, [], {}\n",
    "\n",
    "    return dframe, removed_features, ranking_variance_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding low-variance features.\n",
      "Found 2 low-variance columns.\n",
      "Removing low-variance features.\n",
      "Reassembling the dataframe (with low-variance features removed).\n",
      "Succesfully removed low-variance columns.\n",
      "\n",
      "Low Variance Columns ['public_meeting', 'recorded_by']\n",
      "Shape of X is (59400, 37)\n"
     ]
    }
   ],
   "source": [
    "X, removed_features, ranking_variance_thresholds = get_low_variance_columns(dframe=X,\n",
    "                                                                            threshold=(0.85 * (1 - 0.85)),\n",
    "                                                                            autoremove=True)\n",
    "\n",
    "print '\\nLow Variance Columns', removed_features\n",
    "print 'Shape of X is', X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_X.drop(removed_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is (59400, 37)\n",
      "Shape of TEST_X is (14850, 37)\n"
     ]
    }
   ],
   "source": [
    "print 'Shape of X is', X.shape\n",
    "print 'Shape of TEST_X is', TEST_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best\n",
    "\n",
    "* For regression: f_regression, mutual_info_regression\n",
    "* For classification: chi2, f_classif, mutual_info_classif\n",
    "\n",
    "\n",
    "Random Forest Classifier score: RandomForestClassifier(n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1)\n",
    "* chi2 0.81225589225589223\n",
    "*  f_classic 0.81138047138047142\n",
    "* mutual_info_classif 0.81037037037037041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K_BEST_COLS = 25\n",
    "\n",
    "fit = SelectKBest(score_func=chi2, k=K_BEST_COLS).fit(X, y)\n",
    "cols_names = X.columns\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "ranking_selectkbest = dict(zip(cols_names, fit.scores_))\n",
    "selected_cols =  [_ for _ in cols_names[:K_BEST_COLS]]\n",
    "\n",
    "features = pd.DataFrame(fit.transform(X))\n",
    "features.columns = selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amount_tsh': 74881.491646077397,\n",
       " 'basin': 266.29188142394912,\n",
       " 'construction_year': 24316.883191063331,\n",
       " 'date_recorded': 9583.0078737288022,\n",
       " 'district_code': 644.5891174545709,\n",
       " 'extraction_type': 2638.1965790559675,\n",
       " 'extraction_type_class': 4962.445268561687,\n",
       " 'extraction_type_group': 3427.7617911925518,\n",
       " 'funder': 71952.584999370752,\n",
       " 'gps_height': 512717.9078653045,\n",
       " 'installer': 24259.764104192629,\n",
       " 'latitude': 52365.996981064389,\n",
       " 'lga': 9145.8259153344916,\n",
       " 'longitude': 485645.5061894512,\n",
       " 'management': 176.68024996472877,\n",
       " 'management_group': 30.233212840532243,\n",
       " 'num_private': 210.93553962837058,\n",
       " 'payment': 866.20357165258588,\n",
       " 'payment_type': 462.55917116540058,\n",
       " 'permit': 21.535401607392107,\n",
       " 'population': 5162.4773551716098,\n",
       " 'quality_group': 422.67270817801528,\n",
       " 'quantity': 672.8652286072313,\n",
       " 'quantity_group': 672.8652286072313,\n",
       " 'region': 1805.634614408988,\n",
       " 'region_code': 1704.2139133512608,\n",
       " 'scheme_management': 439.39640906001443,\n",
       " 'scheme_name': 77768.875220045331,\n",
       " 'source': 458.83098562602078,\n",
       " 'source_class': 395.059710920003,\n",
       " 'source_type': 568.22978799330588,\n",
       " 'subvillage': 13625.784062124359,\n",
       " 'ward': 3114.4633187705917,\n",
       " 'water_quality': 0.99149515256397369,\n",
       " 'waterpoint_type': 3348.5174477453156,\n",
       " 'waterpoint_type_group': 2540.8811011376956,\n",
       " 'wpt_name': 117914.64708481921}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % pprint\n",
    "ranking_selectkbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 37) (59400, 30) 59400\n"
     ]
    }
   ],
   "source": [
    "print X.shape, features.shape, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Columns:\n",
      "\tmanagement_group\n",
      "\tpayment\n",
      "\tpayment_type\n",
      "\twater_quality\n",
      "\tquality_group\n",
      "\tquantity\n",
      "\tquantity_group\n",
      "\tsource\n",
      "\tsource_type\n",
      "\tsource_class\n",
      "\twaterpoint_type\n",
      "\twaterpoint_type_group\n",
      "\n",
      "Selected Columns:\n",
      "\tamount_tsh\n",
      "\tdate_recorded\n",
      "\tfunder\n",
      "\tgps_height\n",
      "\tinstaller\n",
      "\tlongitude\n",
      "\tlatitude\n",
      "\twpt_name\n",
      "\tnum_private\n",
      "\tbasin\n",
      "\tsubvillage\n",
      "\tregion\n",
      "\tregion_code\n",
      "\tdistrict_code\n",
      "\tlga\n",
      "\tward\n",
      "\tpopulation\n",
      "\tscheme_management\n",
      "\tscheme_name\n",
      "\tpermit\n",
      "\tconstruction_year\n",
      "\textraction_type\n",
      "\textraction_type_group\n",
      "\textraction_type_class\n",
      "\tmanagement\n"
     ]
    }
   ],
   "source": [
    "print 'Removed Columns:\\n\\t', '\\n\\t'.join([ _ for _ in X.columns if _ not in features.columns ])\n",
    "\n",
    "print '\\nSelected Columns:\\n\\t', '\\n\\t'.join(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 37) (59400, 25)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(fit.transform(X))\n",
    "X.columns = selected_cols\n",
    "\n",
    "print\n",
    "\n",
    "# TEST_X = pd.DataFrame(fit.transform(TEST_X))\n",
    "# TEST_X.columns = selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14850, 25) (59400, 25)\n"
     ]
    }
   ],
   "source": [
    "TEST_X = pd.DataFrame(fit.transform(TEST_X))\n",
    "\n",
    "print TEST_X.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('processed_X.pkl', 'w'))\n",
    "pickle.dump(TEST_X, open('processed_TEST_X.pkl', 'w'))\n",
    "pickle.dump(y, open('processed_y.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('processed_X.pkl'))\n",
    "TEST_X = pickle.load(open('processed_TEST_X.pkl'))\n",
    "y = pickle.load(open('processed_y.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "pca = PCA(n_components=7)\n",
    "fit = pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "_ = plt.scatter (range(len(fit.explained_variance_ratio_)), fit.explained_variance_ratio_.cumsum())\n",
    "\n",
    "_ = plt.xlabel('cumsum of explained variance')\n",
    "\n",
    "print fit.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pca.transform(X)\n",
    "TEST_X = pca.transform(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open('processed_X.pkl', 'w'))\n",
    "pickle.dump(TEST_X, open('processed_TEST_X.pkl', 'w'))\n",
    "pickle.dump(y, open('processed_y.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Unsupervised Learning Exploration(Gaussian Process, Neural Nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('processed_X.pkl'))\n",
    "TEST_X = pickle.load(open('processed_TEST_X.pkl'))\n",
    "y = pickle.load(open('processed_y.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 25) (59400,) (14850, 25)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, y.shape, TEST_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "\n",
    "try:\n",
    "    import renders as rs\n",
    "except:\n",
    "    pass\n",
    "import seaborn as sb\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For future analysis\n",
    "GMM_Centers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=2, n_init=1, precisions_init=None,\n",
       "        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "clusterer = GMM(n_components=i, random_state=42)\n",
    "clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.01e+00   1.76e+02   9.86e+02   4.87e+02   8.30e+02   4.87e+03\n",
      "    5.54e+03   2.52e+04   1.06e+04   1.24e+01   1.31e+01   3.81e+00\n",
      "    5.93e+01   1.03e+03   1.44e+02   1.68e+02   2.07e+01   8.08e+00\n",
      "    5.85e+00   1.39e+00   2.17e+00   1.35e+00   1.35e+00   3.77e+00\n",
      "    2.84e+00]\n",
      " [  1.01e+01   1.96e+02   8.25e+02   8.37e+02   6.59e+02   5.51e+03\n",
      "    5.25e+03   2.47e+04   1.06e+04   8.68e+00   1.01e+01   3.89e+00\n",
      "    6.04e+01   1.07e+03   1.41e+02   9.27e+02   2.64e+01   5.54e+00\n",
      "    3.07e+00   1.21e+00   2.39e+00   1.31e+00   1.31e+00   2.03e+00\n",
      "    1.70e+00]]\n"
     ]
    }
   ],
   "source": [
    "preds = clusterer.predict(X)\n",
    "GMM_Centers.append(clusterer.means_)\n",
    "print clusterer.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032499189887050476"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__check_for  = 1000\n",
    "silhouette_score(X[:__check_for], preds[:__check_for])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.00900773216316\n",
      "3 -0.0319390465026\n",
      "4 -0.172351184764\n",
      "5 -0.139534380156\n",
      "6 -0.231977188575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For future analysis\n",
    "GMM_Centers = []\n",
    "\n",
    "__check_for  = 100\n",
    "\n",
    "for i in range(2, 7):\n",
    "    # TODO: Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer = GMM(n_components=i, random_state=42)\n",
    "    clusterer.fit(X)\n",
    "\n",
    "    # TODO: Predict the cluster for each data point\n",
    "    preds = clusterer.predict(X)\n",
    "\n",
    "    # TODO: Find the cluster centers\n",
    "    GMM_Centers.append(clusterer.means_)\n",
    "\n",
    "    # score = silhouette_score(X, preds)\n",
    "    score = silhouette_score(X[:__check_for], preds[:__check_for])\n",
    "\n",
    "    print i, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.505801576136\n",
      "3 0.334520314762\n",
      "4 0.270235669827\n",
      "5 0.256475256588\n",
      "6 0.249260465848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For future analysis\n",
    "KMM_Centers = []\n",
    "\n",
    "# Testing each category\n",
    "for i in range(2, 7):\n",
    "\n",
    "    clusterer = KMeans(init='k-means++', n_clusters=i, n_init=10)\n",
    "    clusterer.fit(X)\n",
    "\n",
    "    preds = clusterer.predict(X)\n",
    "\n",
    "    centers = clusterer.cluster_centers_\n",
    "    \n",
    "    KMM_Centers.append(centers)\n",
    "\n",
    "#     score = silhouette_score(X, preds)\n",
    "    score = silhouette_score(X[:__check_for], preds[:__check_for])\n",
    "    print i, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "clusterer = KMeans(init='k-means++', n_clusters=i, n_init=10)\n",
    "clusterer.fit(X)\n",
    "preds = clusterer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.495739502324\n"
     ]
    }
   ],
   "source": [
    "score = silhouette_score(X[:1000], preds[:1000])\n",
    "print i, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 25) (14850, 25)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, TEST_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "X['new'] = clusterer.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST_X = pd.DataFrame(TEST_X)\n",
    "TEST_X['new'] = clusterer.predict(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59400, 26) (14850, 26)\n"
     ]
    }
   ],
   "source": [
    "print X.shape, TEST_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Supervised Learning(GBT Trees, Nearest Neighbours, RF, One-vs-One)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# print features.shape\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt_clf = GradientBoostingClassifier(n_estimators=150, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbt_clf = gbt_clf.fit(X_train, y_train)\n",
    "\n",
    "print 'score:', gbt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_clf.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80983164983164979"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1)\n",
    "# class_weight=\"balanced_subsample\"/\"balanced\"\n",
    "# criterion=\"gini\"/\"entropy\"\n",
    "\n",
    "rf_clf = rf_clf.fit(X_train, y_train)\n",
    "# pred = rf_clf.predict_proba(X_test)\n",
    "rf_clf.score(X_test, y_test)\n",
    "\n",
    "# (n_estimators=100, class_weight=\"balanced_subsample\", n_jobs=-1) 0.80782828282828278\n",
    "# (n_estimators=100, class_weight=\"balanced_subsample\", n_jobs=-1) 0.81186868686868685\n",
    "# (n_estimators=150, class_weight=\"balanced_subsample\", n_jobs=-1) 0.8113636363636364\n",
    "\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81018518518518523\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.80858585858585863\n",
    "# (n_estimators=150, criterion='gini', n_jobs=-1) 0.80942760942760938\n",
    "# (n_estimators=150, criterion='entropy', n_jobs=-1) 0.81060606060606055\n",
    "\n",
    "# (n_estimators=10, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.79781144781144786\n",
    "# (n_estimators=10, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.80185185185185182\n",
    "# (n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.8113636363636364\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.80984848484848482\n",
    "\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced\", n_jobs=-1) 0.81259259259259264\n",
    "# (n_estimators=150, criterion='gini', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81259259259259264\n",
    "# (n_estimators=150, criterion='entropy', class_weight=\"balanced_subsample\", n_jobs=-1) 0.81252525252525254\n",
    "\n",
    "# 0.81198653198653203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_rf_clf = OneVsOneClassifier(RandomForestClassifier(n_estimators=100,\n",
    "                                                        criterion='entropy',\n",
    "#                                                         class_weight=\"balanced_subsample\",\n",
    "                                                        n_jobs=-1))\n",
    "\n",
    "# 0.81265993265993264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multiclass_rf_clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=100,\n",
    "                                                        criterion='entropy',\n",
    "                                                        class_weight=\"balanced_subsample\",\n",
    "                                                        n_jobs=-1))\n",
    "\n",
    "# 0.81393939393939396\n",
    "\n",
    "# 0.81272727272727274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81138047138047142"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_rf_clf = multiclass_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "multiclass_rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "            criterion='entropy', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_rf_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [10, 50, 100, 150], 'criterion': ['gini', 'entropy'], 'class_weight': ['balanced_subsample', 'balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [10, 50, 100, 150],\n",
    "    'class_weight': 'balanced_subsample balanced'.split(),\n",
    "    'criterion': 'gini entropy'.split()\n",
    "}\n",
    "GS_CV = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, n_jobs=-1)\n",
    "# GS_CV = RandomizedSearchCV(RandomForestClassifier(), parameters)\n",
    "\n",
    "GS_CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'criterion': 'entropy', 'class_weight': 'balanced'} 0.80595959596\n"
     ]
    }
   ],
   "source": [
    "print GS_CV.best_params_, GS_CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:0.79148,std:0.00165,params:{'n_estimators':10,'criterion':'gini','class_weight':'balanced_subsample'}\n",
      "mean:0.80429,std:0.00242,params:{'n_estimators':50,'criterion':'gini','class_weight':'balanced_subsample'}\n",
      "mean:0.80485,std:0.00225,params:{'n_estimators':100,'criterion':'gini','class_weight':'balanced_subsample'}\n",
      "mean:0.80562,std:0.00227,params:{'n_estimators':150,'criterion':'gini','class_weight':'balanced_subsample'}\n",
      "mean:0.79114,std:0.00215,params:{'n_estimators':10,'criterion':'entropy','class_weight':'balanced_subsample'}\n",
      "mean:0.80273,std:0.00073,params:{'n_estimators':50,'criterion':'entropy','class_weight':'balanced_subsample'}\n",
      "mean:0.80426,std:0.00153,params:{'n_estimators':100,'criterion':'entropy','class_weight':'balanced_subsample'}\n",
      "mean:0.80507,std:0.00238,params:{'n_estimators':150,'criterion':'entropy','class_weight':'balanced_subsample'}\n",
      "mean:0.79249,std:0.00140,params:{'n_estimators':10,'criterion':'gini','class_weight':'balanced'}\n",
      "mean:0.80401,std:0.00248,params:{'n_estimators':50,'criterion':'gini','class_weight':'balanced'}\n",
      "mean:0.80490,std:0.00118,params:{'n_estimators':100,'criterion':'gini','class_weight':'balanced'}\n",
      "mean:0.80498,std:0.00149,params:{'n_estimators':150,'criterion':'gini','class_weight':'balanced'}\n",
      "mean:0.79224,std:0.00108,params:{'n_estimators':10,'criterion':'entropy','class_weight':'balanced'}\n",
      "mean:0.80315,std:0.00197,params:{'n_estimators':50,'criterion':'entropy','class_weight':'balanced'}\n",
      "mean:0.80596,std:0.00045,params:{'n_estimators':100,'criterion':'entropy','class_weight':'balanced'}\n",
      "mean:0.80503,std:0.00134,params:{'n_estimators':150,'criterion':'entropy','class_weight':'balanced'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sampathkumarm/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "for each in GS_CV.grid_scores_:\n",
    "    print str(each).replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "tmp = clf.feature_importances_\n",
    "\n",
    "# making importance relative\n",
    "a, b = min(tmp), max(tmp)\n",
    "cols_imp = (tmp - a) /b\n",
    "_ = plt.scatter(range(30), cols_imp)\n",
    "_ = plt.plot((0, 29), (0.15,0.15), '-r')\n",
    "_ = plt.xlabel('Columns')\n",
    "_ = plt.ylabel('Relative Col Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(lambda x: len(x), [X_test, y_test])\n",
    "\n",
    "clf.score(X_test, y_test) # 0.81097643097643102"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "confusion_maxtrix_stuff(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160498800.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05).fit(X_train, y_train)\n",
    "\n",
    "gbm_predictions = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727811447811\n"
     ]
    }
   ],
   "source": [
    "print sum(gbm_predictions == y_test)/ (1.0 * len(y_test)) # 0.7279461279461279"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "confusion_maxtrix_stuff(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids = RAW_TEST_X.index\n",
    "\n",
    "predictions = clf.predict(TEST_X)\n",
    "\n",
    "print (predictions.shape)\n",
    "\n",
    "predictions_labels = le.inverse_transform(predictions)\n",
    "\n",
    "# sub = pd.DataFrame(predictions, columns=list(le.classes_))\n",
    "sub = pd.DataFrame(predictions_labels, columns=['status_group'])\n",
    "sub.head()\n",
    "\n",
    "sub.insert(loc=0, column='id', value=test_ids)\n",
    "sub.reset_index()\n",
    "sub.to_csv('submit.csv', index = False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
